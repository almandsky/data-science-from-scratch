{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../code-python3\")\n",
    "from collections import Counter, defaultdict\n",
    "from machine_learning import split_data\n",
    "import math, random, re, glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(message):\n",
    "    message = message.lower()                       # convert to lowercase\n",
    "    all_words = re.findall(\"[a-z0-9']+\", message)   # extract the words\n",
    "    return set(all_words)                           # remove duplicates\n",
    "\n",
    "\n",
    "def count_words(training_set):\n",
    "    \"\"\"training set consists of pairs (message, is_spam)\"\"\"\n",
    "    counts = defaultdict(lambda: [0, 0])\n",
    "    for message, is_spam in training_set:\n",
    "        for word in tokenize(message):\n",
    "            counts[word][0 if is_spam else 1] += 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a', 'cow', 'is', 'pig', 'that', 'this'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message = \"this is a cow, that is a pig\"\n",
    "\n",
    "tokenize(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-d56a63b5ece8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcount_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-80cbd4672706>\u001b[0m in \u001b[0;36mcount_words\u001b[0;34m(training_set)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;34m\"\"\"training set consists of pairs (message, is_spam)\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mcounts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_spam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtraining_set\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mcounts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_spam\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "count_words(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/spam_examples/spam/0009.c05e264fbf18783099b53dbc9a9aacda\n",
      "../data/spam_examples/spam/0007.859c901719011d56f8b652ea071c1f8b\n",
      "../data/spam_examples/spam/0010.7f5fb525755c45eb78efc18d7c9ea5aa\n",
      "../data/spam_examples/spam/0001.bfc8d64d12b325ff385cca8d07b84288\n",
      "../data/spam_examples/spam/0005.1f42bb885de0ef7fc5cd09d34dc2ba54\n",
      "../data/spam_examples/spam/0002.24b47bb3ce90708ae29d0aec1da08610\n",
      "../data/spam_examples/spam/0004.1874ab60c71f0b31b580f313a3f6e777\n",
      "../data/spam_examples/spam/0006.7a32642f8c22bbeb85d6c3b5f3890a2c\n",
      "../data/spam_examples/spam/0008.9562918b57e044abfbce260cc875acde\n",
      "../data/spam_examples/spam/0003.4b3d943b8df71af248d12f8b2e7a224a\n",
      "../data/spam_examples/spam/0000.7b1b73cf36cf9dbc3d64e3f2ee2b91f1\n",
      "../data/spam_examples/hard_ham/0009.2bc1d4efa31fc78edb6e4bd82f68023f\n",
      "../data/spam_examples/hard_ham/0003.0aa92b5f121c27c6e094fd89c6c89448\n",
      "../data/spam_examples/hard_ham/0007.7f2ea3a532284cff3321e5ba159cdb50\n",
      "../data/spam_examples/hard_ham/0002.2fe846db6e3249836abdbfcae459bf2a\n",
      "../data/spam_examples/hard_ham/0008.aa277082dc0b40dedc51b2e758a115a9\n",
      "../data/spam_examples/hard_ham/0010.a0f25ac43e31765f180d409984179eab\n",
      "../data/spam_examples/hard_ham/0004.409015be23edf1c42f319a870114664c\n",
      "../data/spam_examples/hard_ham/0005.ebf0581994d57466ee431c7e5cf7794d\n",
      "../data/spam_examples/hard_ham/0001.f0cf04027e74802f09f723cb8916b48e\n",
      "../data/spam_examples/hard_ham/0006.b14155850b5d2dd221534f5595515459\n",
      "../data/spam_examples/easy_ham/0006.ee8b0dba12856155222be180ba122058\n",
      "../data/spam_examples/easy_ham/0005.8c3b9e9c0f3f183ddaf7592a11b99957\n",
      "../data/spam_examples/easy_ham/0002.b3120c4bcbf3101e661161ee7efcb8bf\n",
      "../data/spam_examples/easy_ham/0009.435ae292d75abb1ca492dcc2d5cf1570\n",
      "../data/spam_examples/easy_ham/0007.c75188382f64b090022fa3b095b020b0\n",
      "../data/spam_examples/easy_ham/0001.ea7e79d3153e7469e7a9c3e0af6a357e\n",
      "../data/spam_examples/easy_ham/0010.4996141de3f21e858c22f88231a9f463\n",
      "../data/spam_examples/easy_ham/0004.e8d5727378ddde5c3be181df593f1712\n",
      "../data/spam_examples/easy_ham/0008.20bc0b4ba2d99aae1c7098069f611a9b\n",
      "../data/spam_examples/easy_ham/0003.acfc5ad94bbd27118a0d8685d18c89dd\n",
      "Counter({(False, False): 9, (True, False): 2, (True, True): 1})\n",
      "spammiest_hams [('[Lockergnome Windows Daily]  Brilliant Mistakes', False, 0.014439380471763802), ('[Lockergnome Tech Specialist]  Handprint Singing', False, 0.030439217083233945), ('[Lockergnome Penguin Shell]  Recursive Metaphor', False, 0.030439217083233945), ('CNET NEWS.COM: Cable companies cracking down on Wi-Fi', False, 0.030439217083233945), ('Reg Headlines Wednesday July 10', False, 0.14597617919730502)]\n",
      "hammiest_spams [('FORTUNE 500 COMPANY HIRING, AT HOME REPS.', True, 0.030439217083233945), ('Re: Fw: User Name & Password to Membership To 5 Sites zzzz@example.com pviqg', True, 0.030872634578291747), ('RE: Important Information Concerning Your Bank Account', True, 0.9321093954208443)]\n",
      "spammiest_words [('days', 0.4375, 0.041666666666666664), ('guaranteed', 0.4375, 0.041666666666666664), ('lbs', 0.4375, 0.041666666666666664), ('lose', 0.4375, 0.041666666666666664), ('in', 0.4375, 0.041666666666666664)]\n",
      "hammiest_words [('zzzzteana', 0.0625, 0.375), ('used', 0.0625, 0.2916666666666667), ('make', 0.0625, 0.2916666666666667), ('like', 0.0625, 0.2916666666666667), ('mama', 0.0625, 0.2916666666666667)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def word_probabilities(counts, total_spams, total_non_spams, k=0.5):\n",
    "    \"\"\"turn the word_counts into a list of triplets\n",
    "    w, p(w | spam) and p(w | ~spam)\"\"\"\n",
    "    return [(w,\n",
    "             (spam + k) / (total_spams + 2 * k),\n",
    "             (non_spam + k) / (total_non_spams + 2 * k))\n",
    "             for w, (spam, non_spam) in counts.items()]\n",
    "\n",
    "def spam_probability(word_probs, message):\n",
    "    message_words = tokenize(message)\n",
    "    log_prob_if_spam = log_prob_if_not_spam = 0.0\n",
    "\n",
    "    for word, prob_if_spam, prob_if_not_spam in word_probs:\n",
    "\n",
    "        # for each word in the message,\n",
    "        # add the log probability of seeing it\n",
    "        if word in message_words:\n",
    "            log_prob_if_spam += math.log(prob_if_spam)\n",
    "            log_prob_if_not_spam += math.log(prob_if_not_spam)\n",
    "\n",
    "        # for each word that's not in the message\n",
    "        # add the log probability of _not_ seeing it\n",
    "        else:\n",
    "            log_prob_if_spam += math.log(1.0 - prob_if_spam)\n",
    "            log_prob_if_not_spam += math.log(1.0 - prob_if_not_spam)\n",
    "\n",
    "    prob_if_spam = math.exp(log_prob_if_spam)\n",
    "    prob_if_not_spam = math.exp(log_prob_if_not_spam)\n",
    "    return prob_if_spam / (prob_if_spam + prob_if_not_spam)\n",
    "\n",
    "\n",
    "class NaiveBayesClassifier:\n",
    "\n",
    "    def __init__(self, k=0.5):\n",
    "        self.k = k\n",
    "        self.word_probs = []\n",
    "\n",
    "    def train(self, training_set):\n",
    "\n",
    "        # count spam and non-spam messages\n",
    "        num_spams = len([is_spam\n",
    "                         for message, is_spam in training_set\n",
    "                         if is_spam])\n",
    "        num_non_spams = len(training_set) - num_spams\n",
    "\n",
    "        # run training data through our \"pipeline\"\n",
    "        word_counts = count_words(training_set)\n",
    "        self.word_probs = word_probabilities(word_counts,\n",
    "                                             num_spams,\n",
    "                                             num_non_spams,\n",
    "                                             self.k)\n",
    "\n",
    "    def classify(self, message):\n",
    "        return spam_probability(self.word_probs, message)\n",
    "\n",
    "\n",
    "def get_subject_data(path):\n",
    "\n",
    "    data = []\n",
    "\n",
    "    # regex for stripping out the leading \"Subject:\" and any spaces after it\n",
    "    subject_regex = re.compile(r\"^Subject:\\s+\")\n",
    "\n",
    "    # glob.glob returns every filename that matches the wildcarded path\n",
    "    for fn in glob.glob(path):\n",
    "        is_spam = \"ham\" not in fn\n",
    "\n",
    "        with open(fn,'r',encoding='ISO-8859-1') as file:\n",
    "            for line in file:\n",
    "                if line.startswith(\"Subject:\"):\n",
    "                    subject = subject_regex.sub(\"\", line).strip()\n",
    "                    data.append((subject, is_spam))\n",
    "\n",
    "    print(data)\n",
    "    return data\n",
    "\n",
    "def p_spam_given_word(word_prob):\n",
    "    word, prob_if_spam, prob_if_not_spam = word_prob\n",
    "    return prob_if_spam / (prob_if_spam + prob_if_not_spam)\n",
    "\n",
    "def train_and_test_model(path):\n",
    "\n",
    "    data = get_subject_data(path)\n",
    "    random.seed(0)      # just so you get the same answers as me\n",
    "    train_data, test_data = split_data(data, 0.75)\n",
    "\n",
    "    classifier = NaiveBayesClassifier()\n",
    "    classifier.train(train_data)\n",
    "\n",
    "    classified = [(subject, is_spam, classifier.classify(subject))\n",
    "              for subject, is_spam in test_data]\n",
    "\n",
    "    counts = Counter((is_spam, spam_probability > 0.5) # (actual, predicted)\n",
    "                     for _, is_spam, spam_probability in classified)\n",
    "\n",
    "    print(counts)\n",
    "\n",
    "    classified.sort(key=lambda row: row[2])\n",
    "    spammiest_hams = list(filter(lambda row: not row[1], classified))[-5:]\n",
    "    hammiest_spams = list(filter(lambda row: row[1], classified))[:5]\n",
    "\n",
    "    print(\"spammiest_hams\", spammiest_hams)\n",
    "    print(\"hammiest_spams\", hammiest_spams)\n",
    "\n",
    "    words = sorted(classifier.word_probs, key=p_spam_given_word)\n",
    "\n",
    "    spammiest_words = words[-5:]\n",
    "    hammiest_words = words[:5]\n",
    "\n",
    "    print(\"spammiest_words\", spammiest_words)\n",
    "    print(\"hammiest_words\", hammiest_words)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #train_and_test_model(r\"c:\\spam\\*\\*\")\n",
    "    train_and_test_model(r\"../data/spam_examples/*/*\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
