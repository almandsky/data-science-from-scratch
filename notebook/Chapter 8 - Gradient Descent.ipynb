{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../code-python3\")\n",
    "\n",
    "from collections import Counter\n",
    "from linear_algebra import distance, vector_subtract, scalar_multiply\n",
    "from functools import reduce\n",
    "import math, random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating the Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10c8d4438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def sum_of_squares(v):\n",
    "    \"\"\"computes the sum of squared elements in v\"\"\"\n",
    "    return sum(v_i ** 2 for v_i in v)\n",
    "\n",
    "def difference_quotient(f, x, h):\n",
    "    return (f(x + h) - f(x)) / h\n",
    "\n",
    "def plot_estimated_derivative():\n",
    "\n",
    "    def square(x):\n",
    "        return x * x\n",
    "\n",
    "    def derivative(x):\n",
    "        return 2 * x\n",
    "\n",
    "    derivative_estimate = lambda x: difference_quotient(square, x, h=0.00001)\n",
    "\n",
    "    # plot to show they're basically the same\n",
    "    import matplotlib.pyplot as plt\n",
    "    x = range(-10,10)\n",
    "    # In Python 3, map returns an iterator not a list\n",
    "    # we need to convert the map to list below\n",
    "    plt.plot(x, list(map(derivative, x)), 'rx')           # red  x\n",
    "    plt.plot(x, list(map(derivative_estimate, x)), 'b+')  # blue +\n",
    "    plt.show()                                      # purple *, hopefully\n",
    "    \n",
    "plot_estimated_derivative()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_difference_quotient(f, v, i, h):\n",
    "\n",
    "    # add h to just the i-th element of v\n",
    "    w = [v_j + (h if j == i else 0)\n",
    "         for j, v_j in enumerate(v)]\n",
    "\n",
    "    return (f(w) - f(v)) / h\n",
    "\n",
    "def estimate_gradient(f, v, h=0.00001):\n",
    "    return [partial_difference_quotient(f, v, i, h)\n",
    "            for i, _ in enumerate(v)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using the gradient\n",
      "minimum v [-4.443025454555154e-07, 2.2215127272775794e-06, 4.443025454555159e-06]\n",
      "minimum value 2.4872998739179593e-11\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def step(v, direction, step_size):\n",
    "    \"\"\"move step_size in the direction from v\"\"\"\n",
    "    return [v_i + step_size * direction_i\n",
    "            for v_i, direction_i in zip(v, direction)]\n",
    "\n",
    "def sum_of_squares_gradient(v):\n",
    "    return [2 * v_i for v_i in v]\n",
    "\n",
    "\n",
    "print(\"using the gradient\")\n",
    "\n",
    "v = [random.randint(-10,10) for i in range(3)]\n",
    "\n",
    "tolerance = 0.0000001\n",
    "\n",
    "while True:\n",
    "    #print v, sum_of_squares(v)\n",
    "    gradient = sum_of_squares_gradient(v)   # compute the gradient at v\n",
    "    next_v = step(v, gradient, -0.01)       # take a negative gradient step\n",
    "    if distance(next_v, v) < tolerance:     # stop if we're converging\n",
    "        break\n",
    "    v = next_v                              # continue if we're not\n",
    "\n",
    "print(\"minimum v\", v)\n",
    "print(\"minimum value\", sum_of_squares(v))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting It All Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using minimize_batch\n",
      "minimum v [0.00010633823966279331, -0.0009570441569651392, 0.001063382396627933]\n",
      "minimum value 2.0580234610538627e-06\n"
     ]
    }
   ],
   "source": [
    "def safe(f):\n",
    "    \"\"\"define a new function that wraps f and return it\"\"\"\n",
    "    def safe_f(*args, **kwargs):\n",
    "        try:\n",
    "            return f(*args, **kwargs)\n",
    "        except:\n",
    "            return float('inf')         # this means \"infinity\" in Python\n",
    "    return safe_f\n",
    "\n",
    "\n",
    "#\n",
    "#\n",
    "# minimize / maximize batch\n",
    "#\n",
    "#\n",
    "\n",
    "def minimize_batch(target_fn, gradient_fn, theta_0, tolerance=0.000001):\n",
    "    \"\"\"use gradient descent to find theta that minimizes target function\"\"\"\n",
    "\n",
    "    step_sizes = [100, 10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "\n",
    "    theta = theta_0                           # set theta to initial value\n",
    "    target_fn = safe(target_fn)               # safe version of target_fn\n",
    "    value = target_fn(theta)                  # value we're minimizing\n",
    "\n",
    "    while True:\n",
    "        gradient = gradient_fn(theta)\n",
    "        next_thetas = [step(theta, gradient, -step_size)\n",
    "                       for step_size in step_sizes]\n",
    "\n",
    "        # choose the one that minimizes the error function\n",
    "        next_theta = min(next_thetas, key=target_fn)\n",
    "        next_value = target_fn(next_theta)\n",
    "\n",
    "        # stop if we're \"converging\"\n",
    "        if abs(value - next_value) < tolerance:\n",
    "            return theta\n",
    "        else:\n",
    "            theta, value = next_theta, next_value\n",
    "\n",
    "def negate(f):\n",
    "    \"\"\"return a function that for any input x returns -f(x)\"\"\"\n",
    "    return lambda *args, **kwargs: -f(*args, **kwargs)\n",
    "\n",
    "def negate_all(f):\n",
    "    \"\"\"the same when f returns a list of numbers\"\"\"\n",
    "    return lambda *args, **kwargs: [-y for y in f(*args, **kwargs)]\n",
    "\n",
    "def maximize_batch(target_fn, gradient_fn, theta_0, tolerance=0.000001):\n",
    "    return minimize_batch(negate(target_fn),\n",
    "                          negate_all(gradient_fn),\n",
    "                          theta_0,\n",
    "                          tolerance)\n",
    "\n",
    "\n",
    "print(\"using minimize_batch\")\n",
    "\n",
    "v = [random.randint(-10,10) for i in range(3)]\n",
    "\n",
    "v = minimize_batch(sum_of_squares, sum_of_squares_gradient, v)\n",
    "\n",
    "print(\"minimum v\", v)\n",
    "print(\"minimum value\", sum_of_squares(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using minimize_stochastic\n",
      "minimum v -9.916356901856894\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# minimize / maximize stochastic\n",
    "#\n",
    "\n",
    "def in_random_order(data):\n",
    "    \"\"\"generator that returns the elements of data in random order\"\"\"\n",
    "    indexes = [i for i, _ in enumerate(data)]  # create a list of indexes\n",
    "    random.shuffle(indexes)                    # shuffle them\n",
    "    for i in indexes:                          # return the data in that order\n",
    "        yield data[i]\n",
    "\n",
    "def minimize_stochastic(target_fn, gradient_fn, x, y, theta_0, alpha_0=0.01):\n",
    "\n",
    "    data = list(zip(x, y))\n",
    "    theta = theta_0                             # initial guess\n",
    "    alpha = alpha_0                             # initial step size\n",
    "    min_theta, min_value = None, float(\"inf\")   # the minimum so far\n",
    "    iterations_with_no_improvement = 0\n",
    "\n",
    "    # if we ever go 100 iterations with no improvement, stop\n",
    "    while iterations_with_no_improvement < 100:\n",
    "        value = sum( target_fn(x_i, y_i, theta) for x_i, y_i in data )\n",
    "\n",
    "        if value < min_value:\n",
    "            # if we've found a new minimum, remember it\n",
    "            # and go back to the original step size\n",
    "            min_theta, min_value = theta, value\n",
    "            iterations_with_no_improvement = 0\n",
    "            alpha = alpha_0\n",
    "        else:\n",
    "            # otherwise we're not improving, so try shrinking the step size\n",
    "            iterations_with_no_improvement += 1\n",
    "            alpha *= 0.9\n",
    "\n",
    "        # and take a gradient step for each of the data points\n",
    "        for x_i, y_i in in_random_order(data):\n",
    "            gradient_i = gradient_fn(x_i, y_i, theta)\n",
    "            # theta = vector_subtract(theta, scalar_multiply(alpha, gradient_i))\n",
    "            theta = theta - alpha * gradient_i\n",
    "\n",
    "    return min_theta\n",
    "\n",
    "def maximize_stochastic(target_fn, gradient_fn, x, y, theta_0, alpha_0=0.01):\n",
    "    return minimize_stochastic(negate(target_fn),\n",
    "                               negate_all(gradient_fn),\n",
    "                               x, y, theta_0, alpha_0)\n",
    "\n",
    "\n",
    "print(\"using minimize_stochastic\")\n",
    "\n",
    "def targetfunc(x, y, theta):\n",
    "    return x**2 + y*theta\n",
    "\n",
    "def gradientfunc(x, y, theta):\n",
    "    return x*2 + y*theta\n",
    "\n",
    "v = [random.randint(-10,10) for i in range(3)]\n",
    "\n",
    "x = [random.randint(-100,100) for i in range(100)]\n",
    "y = [random.randint(-100,100) for i in range(100)]\n",
    "\n",
    "v = minimize_stochastic(targetfunc, gradientfunc, x, y, 0.01)\n",
    "\n",
    "print(\"minimum v\", v)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
